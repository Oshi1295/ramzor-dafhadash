# -*- coding: utf-8 -*-
"""×¤×¨×•×™×§×˜_×“××˜×”_×‘×¢×•×œ×_×”×¢×¡×§×™ (3) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18dfPjQU18Zgdolo78HTm6G4oBGiEfw5Y
"""

!pip install pymupdf

!pip install fitz  tools

!pip install pdfplumber

import pymupdf as fitz # PyMuPDF
import re
import pandas as pd
import numpy as np
import traceback
from google.colab import files # In Colab
from IPython.display import display
import ipywidgets as widgets


COLUMN_HEADER_WORDS = {
    "×©×", "××§×•×¨", "××™×“×¢", "××“×•×•×—", "××–×”×”", "×¢×¡×§×”", "××¡×¤×¨", "×¢×¡×§××•×ª",
    "×’×•×‘×”", "××¡×’×¨×ª", "××¡×’×¨×•×ª", "×¡×›×•×", "×”×œ×•×•××•×ª", "××§×•×¨×™", "×™×ª×¨×ª", "×—×•×‘",
    "×™×ª×¨×”", "×©×œ×", "×©×•×œ××”", "×‘××•×¢×“"
}
BANK_KEYWORDS = {"×‘× ×§", "×‘×¢\"×", "××’×•×“", "×“×™×¡×§×•× ×˜", "×œ××•××™", "×”×¤×•×¢×œ×™×", "××–×¨×—×™",
                 "×˜×¤×—×•×ª", "×”×‘×™× ×œ××•××™", "××¨×›× ×ª×™×œ", "××•×¦×¨", "×”×—×™×™×œ", "×™×¨×•×©×œ×™×",
                 "××™×’×•×“", "××™××•×Ÿ", "×™×©×™×¨", "×›×¨×˜×™×¡×™", "××©×¨××™", "××§×¡", "×¤×™× × ×¡×™×",
                 "×›××œ", "×™×©×¨××›×¨×˜"}

def process_entry_final(entry_data, section, all_rows_list):
    """
    Processes a completed entry (bank + numbers) and adds it to the list.
    Determines unpaid=0.0 based on collected count and performs final name cleaning.
    Handles potential missing 'processed' key gracefully.
    """
    if not entry_data or not entry_data.get('bank') or len(entry_data.get('numbers', [])) < 2: return

    bank_name_raw = entry_data['bank']
    # More aggressive cleaning of IDs potentially appended to bank names
    bank_name_cleaned = re.sub(r'\s*XX-[\w\d\-]+.*', '', bank_name_raw).strip() # Remove ID and anything after
    bank_name_cleaned = re.sub(r'\s+\d{1,3}(?:,\d{3})*$', '', bank_name_cleaned).strip() # Remove trailing numbers that might be amounts
    bank_name_cleaned = re.sub(r'\s+×‘×¢\"×$', '', bank_name_cleaned).strip() # Remove trailing ×‘×¢"× if it got duplicated
    bank_name_final = bank_name_cleaned if bank_name_cleaned else bank_name_raw # Fallback if cleaning removed everything

    # Ensure bank name ends with ×‘×¢"× if it's likely a bank, unless it's a specific non-bank entity
    is_likely_bank = any(kw in bank_name_final for kw in ["×‘× ×§", "×œ××•××™", "×”×¤×•×¢×œ×™×", "×“×™×¡×§×•× ×˜", "××–×¨×—×™", "×”×‘×™× ×œ××•××™", "××¨×›× ×ª×™×œ", "×™×¨×•×©×œ×™×", "××™×’×•×“"])
    is_non_bank_entity = any(kw in bank_name_final for kw in ["××™××•×Ÿ ×™×©×™×¨", "××§×¡ ××™×˜", "×›×¨×˜×™×¡×™ ××©×¨××™", "×›××œ", "×™×©×¨××›×¨×˜"])

    if is_likely_bank and not bank_name_final.endswith("×‘×¢\"×"):
        bank_name_final += " ×‘×¢\"×"
    elif is_non_bank_entity and not bank_name_final.endswith("×‘×¢\"×"):
         # Check specific non-bank entities that DO end with ×‘×¢"×
         if any(kw in bank_name_final for kw in ["××§×¡ ××™×˜ ×¤×™× × ×¡×™×", "××™××•×Ÿ ×™×©×™×¨ × ×“×œ\"×Ÿ ×•××©×›× ×ª××•×ª"]):
              bank_name_final += " ×‘×¢\"×"


    numbers = entry_data['numbers']
    num_count = len(numbers)

    # Assign numbers based on section type and count
    limit_col, original_col, outstanding_col, unpaid_col = np.nan, np.nan, np.nan, np.nan

    if num_count >= 2:
        val1 = numbers[0]
        val2 = numbers[1]
        val3 = numbers[2] if num_count >= 3 else 0.0 # Default unpaid to 0

        if section in ["×¢×•\"×©", "××¡×’×¨×ª ××©×¨××™"]:
            limit_col = val1 # First number is limit/framework
            outstanding_col = val2 # Second number is outstanding balance
            unpaid_col = val3 # Third is unpaid (if exists)
        elif section in ["×”×œ×•×•××”", "××©×›× ×ª×”"]:
            # Handle different orderings seen in samples
            # Sometimes it's NumTransactions, Original, Outstanding, Unpaid
            # Sometimes just Original, Outstanding, Unpaid
            # Let's assume the first number could be num_transactions if it's small (<50?)
            # Or it could be Original amount
            # Heuristic: If first number is small (<50) AND 4 numbers exist -> skip first
            # Heuristic: If first number is large -> assume Original
            if num_count >= 3: # Need at least Original, Outstanding, Unpaid
                 # If the first number looks like 'num transactions' (small integer) and there are actually 4 numbers
                 if val1 < 50 and val1 == int(val1) and num_count >= 4:
                      original_col = numbers[1]
                      outstanding_col = numbers[2]
                      unpaid_col = numbers[3]
                 # If the first number looks like 'num transactions' but only 3 numbers exist (Original likely missing)
                 # This case is ambiguous, let's assume standard order: Original, Outstanding, Unpaid
                 # So if val1 is small, it might be Original is small, or data is weird. Stick to order.
                 #elif val1 < 50 and val1 == int(val1) and num_count == 3:
                 #     original_col = np.nan # Or assume it's the original? Ambiguous. Let's assume order.
                 #     outstanding_col = numbers[1]
                 #     unpaid_col = numbers[2]
                 # Standard case: Original, Outstanding, Unpaid
                 else:
                     original_col = val1
                     outstanding_col = val2
                     unpaid_col = val3

            # Fallback if only 2 numbers provided for loan/mortgage (unlikely but possible)
            elif num_count == 2:
                 original_col = val1 # Assume original
                 outstanding_col = val2 # Assume outstanding
                 unpaid_col = 0.0 # Default unpaid

        else: # Unknown section type - make a best guess
            original_col = val1
            outstanding_col = val2
            unpaid_col = val3

        all_rows_list.append({
            "×¡×•×’ ×¢×¡×§×”": section,
            "×©× ×‘× ×§/××§×•×¨": bank_name_final,
            "×’×•×‘×” ××¡×’×¨×ª": limit_col,
            "×¡×›×•× ××§×•×¨×™": original_col,
            "×™×ª×¨×ª ×—×•×‘": outstanding_col,
            "×™×ª×¨×” ×©×œ× ×©×•×œ××”": unpaid_col
        })


def extract_credit_data_final_v13(pdf_path):
    """
    Extracts credit data tables from all pages, handling consecutive entries
    for the same bank separated by an ID line (XX-...). V13 Logic.
    """
    extracted_rows = []
    print(f"\n--- Starting V13 Extraction for: {pdf_path} ---")
    try:
        with fitz.open(pdf_path) as doc:
            current_section = None
            current_entry = None # Dict: {'bank': str, 'numbers': list, 'processed': bool}
            last_line_was_id = False # Flag to detect ID line right before numbers
            potential_bank_continuation_candidate = False # Helps link multi-line bank names

            section_patterns = {
                 # Order matters slightly - check longer phrases first if overlapping
                "×—×©×‘×•×Ÿ ×¢×•×‘×¨ ×•×©×‘": "×¢×•\"×©", # Page 3
                "×”×œ×•×•××”": "×”×œ×•×•××”", # Page 3
                "××©×›× ×ª×”": "××©×›× ×ª×”", # Page 3
                "××¡×’×¨×ª ××©×¨××™ ××ª×—×“×©×ª": "××¡×’×¨×ª ××©×¨××™", # Page 3
                # Add other potential section headers if they exist in full reports
            }
            number_line_pattern = re.compile(r"^\s*(-?\d{1,3}(?:,\d{3})*\.?\d*)\s*$") # Allow negative numbers

            for page_num, page in enumerate(doc):
                # print(f"\n--- Processing Page {page_num + 1} ---")
                text = page.get_text("text")
                lines = text.splitlines()
                # Reset continuation candidate at start of page? Maybe not needed if state persists correctly.

                for i, line in enumerate(lines):
                    original_line = line; line = line.strip()
                    # print(f"D Line {i}: '{line}' | Sect: {current_section} | Entry: {current_entry} | LastID: {last_line_was_id} | Cont: {potential_bank_continuation_candidate}") # Verbose Debug

                    if not line:
                        potential_bank_continuation_candidate = False # Empty line breaks continuation
                        continue

                    # --- Step 1: Check for lines that finalize the current entry AND change state ---
                    is_section_header = False
                    # Check for section headers using the defined patterns
                    for header_keyword, section_name in section_patterns.items():
                         # Be more robust: Check if the line *contains* the keyword AND is relatively short / looks like a title
                        if header_keyword in line and len(line) < len(header_keyword) + 20: # Allow some flexibility
                            # Avoid matching if it's clearly part of a sentence or different context
                            if line.count(' ') < 5 : # Simple check: headers usually have few words
                                # print(f"  Found Section Header: {line}") # Debug
                                if current_entry and not current_entry.get('processed', False):
                                    # print(f"    Processing entry before section change: {current_entry}") # Debug
                                    process_entry_final(current_entry, current_section, extracted_rows)
                                    # No need to mark processed here, it's being replaced
                                current_section = section_name
                                current_entry = None # Reset entry for new section
                                last_line_was_id = False
                                potential_bank_continuation_candidate = False
                                is_section_header = True
                                break
                    if is_section_header: continue

                    is_total_line = line.startswith("×¡×”\"×›")
                    if is_total_line:
                        # print(f"  Found Total Line: {line}") # Debug
                        if current_entry and not current_entry.get('processed', False):
                            # print(f"    Processing entry before total line: {current_entry}") # Debug
                            process_entry_final(current_entry, current_section, extracted_rows)
                        current_entry = None # Reset entry, totals are separate
                        last_line_was_id = False
                        potential_bank_continuation_candidate = False
                        continue # Skip processing the total line itself

                    # --- Step 2: Process line content if within a known section ---
                    if current_section:
                        number_match = number_line_pattern.match(line)
                        is_id_line = line.startswith("XX-") and len(line) > 5 # Basic ID check
                        is_header_word = any(word == line for word in COLUMN_HEADER_WORDS)
                        # Noise = column header words OR isolated punctuation/short meaningless strings
                        is_noise_line = is_header_word or line in [':', '.'] or (len(line)<3 and not line.isdigit())

                        # 2a. Handle Number Line
                        if number_match:
                            # print(f"  Number Line: {line}") # Debug
                            if current_entry:
                                try:
                                    number = float(number_match.group(1).replace(",", ""))
                                    num_list = current_entry.get('numbers', [])

                                    # --- V13 Logic: Check for implicit new entry ---
                                    # If the last line was an ID AND we already have >= 2 numbers,
                                    # this number likely starts a NEW entry for the SAME bank.
                                    if last_line_was_id and len(num_list) >= 2:
                                        # print(f"    Detected number after ID, processing previous entry: {current_entry}") # Debug
                                        if not current_entry.get('processed', False):
                                             process_entry_final(current_entry, current_section, extracted_rows)
                                             # Mark old entry as processed to avoid reprocessing? Risky if structure varies. Let process_entry handle duplicates if needed.

                                        # Start new entry with same bank, this number is the first for it
                                        # print(f"    Starting new implicit entry for bank '{current_entry['bank']}' with number {number}") # Debug
                                        new_entry = {'bank': current_entry['bank'], 'numbers': [number], 'processed': False}
                                        current_entry = new_entry # Replace current_entry
                                    else:
                                        # Normal case: Add number to current entry if list has space (e.g., < 4 numbers)
                                        # Let's allow up to 4 numbers to capture potential num_transactions
                                        if len(num_list) < 4:
                                            current_entry['numbers'].append(number)
                                            # print(f"    Added number. Entry numbers now: {current_entry['numbers']}") # Debug
                                        #else: print(f"    Number ignored, already have {len(num_list)} numbers.")

                                except ValueError: pass # Ignore if conversion fails
                            # else: print("   Number line ignored, no current entry.") # Debug
                            last_line_was_id = False # Reset ID flag after a number
                            potential_bank_continuation_candidate = False # Numbers break bank name continuation
                            continue # Move to next line

                        # 2b. Handle ID Line
                        elif is_id_line:
                            # print(f"  ID Line: {line}") # Debug
                            last_line_was_id = True # Set flag
                            potential_bank_continuation_candidate = False # ID breaks bank name continuation
                            continue # Don't process entry based on ID alone, wait for next line

                        # 2c. Handle Noise Line (Column Headers, Punctuation etc)
                        elif is_noise_line:
                            # print(f"  Noise Line: {line}") # Debug
                            # Noise generally signifies a break, but let's not process the entry here yet.
                            # Wait for a definitive boundary like a new bank or section header.
                            # Just reset flags.
                            last_line_was_id = False
                            potential_bank_continuation_candidate = False
                            continue

                        # 2d. Handle Potential Bank Name / Continuation / Other Text
                        else:
                             # print(f"  Potential Bank/Text Line: {line}") # Debug
                             cleaned_line_for_kw_check = re.sub(r'\s*XX-[\w\d\-]+.*', '', line).strip() # Clean potential trailing ID
                             cleaned_line_for_kw_check = re.sub(r'\d+$', '', cleaned_line_for_kw_check).strip() # Clean potential trailing number
                             contains_keyword = any(kw in cleaned_line_for_kw_check for kw in BANK_KEYWORDS)
                             # Consider it a potential bank if it has a keyword OR is reasonably long
                             is_potential_bank = contains_keyword or len(cleaned_line_for_kw_check) > 6

                             # Check for continuation: If the flag is set AND current entry has no numbers yet
                             # AND the line starts with a common continuation phrase.
                             common_continuations = ["×œ×™×©×¨××œ", "×‘×¢\"×", "×•××©×›× ×ª××•×ª", "× ×“×œ\"×Ÿ", "×“×™×¡×§×•× ×˜", "×”×¨××©×•×Ÿ", "×¤×™× × ×¡×™×"]
                             is_continuation = (potential_bank_continuation_candidate and current_entry and
                                                not current_entry.get('numbers') and # Only continue if no numbers collected yet
                                                any(cleaned_line_for_kw_check.startswith(cont) for cont in common_continuations))

                             if is_continuation:
                                 # print(f"    Appending continuation: '{cleaned_line_for_kw_check}'") # Debug
                                 appendix = cleaned_line_for_kw_check
                                 if appendix:
                                     current_entry['bank'] += " " + appendix
                                     current_entry['bank'] = current_entry['bank'].replace(" ×‘×¢\"× ×‘×¢\"×", " ×‘×¢\"×") # Prevent duplication
                                 potential_bank_continuation_candidate = True # Allow multi-word continuation
                             elif is_potential_bank:
                                 # Looks like a *new* bank name (or the start of one)
                                 # print(f"    Detected as New Bank Line: {line}") # Debug
                                 # Process the previous entry *before* starting the new one
                                 if current_entry and not current_entry.get('processed', False):
                                     # print(f"      Processing previous entry: {current_entry}") # Debug
                                     process_entry_final(current_entry, current_section, extracted_rows)
                                     # Don't mark processed, just let it be replaced

                                 # Start the new entry
                                 # print(f"      Starting new entry with bank: {line}") # Debug
                                 current_entry = {'bank': line, 'numbers': [], 'processed': False}
                                 potential_bank_continuation_candidate = True # This new line might be continued
                             else:
                                 # It's neither a known continuation nor looks like a bank name starter.
                                 # Treat as other noise/unexpected text - ignore for now.
                                 # print(f"    Line ignored as other text: {line}") # Debug
                                 potential_bank_continuation_candidate = False # Break continuation chain

                             last_line_was_id = False # Reset ID flag if it wasn't an ID line

                    # --- End of Step 2 (within section) ---
                # --- End of line loop (inside page) ---
            # --- End of page loop ---

            # --- Final processing after all pages ---
            # print("\n--- Debug: Finished processing all pages (V13) ---")
            if current_entry and not current_entry.get('processed', False):
                # print("--- Processing final pending entry ---")
                # print(f"  Final entry data: {current_entry}") # Debug
                process_entry_final(current_entry, current_section, extracted_rows)

    except Exception as e: print(f"FATAL ERROR: {e}"); traceback.print_exc(); return pd.DataFrame()

    # Create DataFrame
    # print(f"\n--- Extracted {len(extracted_rows)} rows in total (V13) ---")
    if not extracted_rows: print("--- No rows extracted. ---"); return pd.DataFrame()
    # Ensure correct order and handle potential missing columns gracefully later if needed
    df = pd.DataFrame(extracted_rows)
    final_cols = ["×¡×•×’ ×¢×¡×§×”", "×©× ×‘× ×§/××§×•×¨", "×’×•×‘×” ××¡×’×¨×ª", "×¡×›×•× ××§×•×¨×™", "×™×ª×¨×ª ×—×•×‘", "×™×ª×¨×” ×©×œ× ×©×•×œ××”"]
    # Fill missing columns with NaN and reorder
    for col in final_cols:
        if col not in df.columns:
            df[col] = np.nan
    df = df[final_cols]
    print("--- Final DataFrame created successfully (V13) ---")
    return df

# --- ×¤×•× ×§×¦×™×™×ª ×¡×™×›×•× (×œ×œ× ×©×™× ×•×™) ---
def calculate_summary(df_extracted, estimated_monthly_income=12000):
    if df_extracted.empty: print("Cannot calculate summary: Empty DataFrame."); return pd.DataFrame(columns=["×¤×¨××˜×¨", "×¢×¨×š"])
    print("--- Calculating financial summary... ---")
    # Ensure numeric types before summing, coercing errors to NaN
    for col in ["×’×•×‘×” ××¡×’×¨×ª", "×¡×›×•× ××§×•×¨×™", "×™×ª×¨×ª ×—×•×‘", "×™×ª×¨×” ×©×œ× ×©×•×œ××”"]:
         if col in df_extracted.columns:
              df_extracted[col] = pd.to_numeric(df_extracted[col], errors='coerce')

    total_original = df_extracted["×¡×›×•× ××§×•×¨×™"].sum(skipna=True)
    total_limit = df_extracted["×’×•×‘×” ××¡×’×¨×ª"].sum(skipna=True)
    total_outstanding = df_extracted["×™×ª×¨×ª ×—×•×‘"].sum(skipna=True)
    total_unpaid = df_extracted["×™×ª×¨×” ×©×œ× ×©×•×œ××”"].sum(skipna=True)

    # Refined total commitments calculation based on column definitions
    # Sum 'Original Amount' for Loans/Mortgages (represents initial commitment)
    loan_mortgage_original = df_extracted.loc[df_extracted['×¡×•×’ ×¢×¡×§×”'].isin(['×”×œ×•×•××”', '××©×›× ×ª×”']), '×¡×›×•× ××§×•×¨×™'].sum(skipna=True)
    # Sum 'Outstanding Balance' for O/S and Credit Lines (represents current used part of facility)
    os_credit_outstanding = df_extracted.loc[df_extracted['×¡×•×’ ×¢×¡×§×”'].isin(['×¢×•\"×©', '××¡×’×¨×ª ××©×¨××™']), '×™×ª×¨×ª ×—×•×‘'].sum(skipna=True)
    # Total commitments could be interpreted differently, let's stick to outstanding for ratio
    # total_commitments = loan_mortgage_original + os_credit_outstanding # This mixes original and outstanding

    disposable_income = estimated_monthly_income * 0.5 # Simple 50% estimate
    # Debt-to-income using total outstanding debt vs annual income
    debt_to_income_ratio = (total_outstanding / (estimated_monthly_income * 12)) if estimated_monthly_income > 0 else 0

    summary_dict = {
        "×¡×š ×¡×›×•× ××§×•×¨×™ (×”×œ×•×•××•×ª/××©×›× ×ª××•×ª)": loan_mortgage_original,
        "×¡×š ×’×•×‘×” ××¡×’×¨×•×ª (×¢×•\"×©/××©×¨××™)": total_limit,
        "×¡×š ×™×ª×¨×•×ª ×—×•×‘ × ×•×›×—×™×•×ª (×›×œ ×”×¡×•×’×™×)": total_outstanding,
        "×¡×”\"×› ×—×•×‘ ×©×œ× ×©×•×œ× ×‘×–××Ÿ": total_unpaid,
        "×”×›× ×¡×” ×—×•×“×©×™×ª (××©×•×¢×¨×ª)": estimated_monthly_income,
        #"×”×›× ×¡×” ×¤× ×•×™×” (×”×¢×¨×›×”)": disposable_income, # Commented out as it's a rough estimate
        "×™×—×¡ ×—×•×‘ ×œ×”×›× ×¡×” ×©× ×ª×™×ª (××©×•×¢×¨×ª)": f"{debt_to_income_ratio:.2%}" if estimated_monthly_income > 0 else "N/A" # Format as percentage
    }
    print("--- Summary calculation complete. ---")
    return pd.DataFrame(list(summary_dict.items()), columns=["×¤×¨××˜×¨", "×¢×¨×š"])


# --- ×§×•×“ ×”×¨×¦×” ×¨××©×™ ---
print("Please upload the PDF file:")
uploaded = files.upload() # Ensure this runs in an environment like Colab

if not uploaded:
    print("No file uploaded. Exiting.")
else:
    for filename in uploaded.keys():
        print(f"\nğŸ“‚ Processing file: {filename}")
        # --- ×©×™××•×© ×‘×¤×•× ×§×¦×™×” V13 ×”××¢×•×“×›× ×ª ---
        df_extracted = extract_credit_data_final_v13(filename)

        if not df_extracted.empty:
            print("\nğŸ“Š Extracted Debt Table (V13):\n")
            # Display with formatting for better readability
            display(df_extracted.style.format({
                "×’×•×‘×” ××¡×’×¨×ª": '{:,.0f}',
                "×¡×›×•× ××§×•×¨×™": '{:,.0f}',
                "×™×ª×¨×ª ×—×•×‘": '{:,.0f}',
                "×™×ª×¨×” ×©×œ× ×©×•×œ××”": '{:,.0f}'
            }))

            # --- ×§×œ×˜ ×”×›× ×¡×” ×•×—×™×©×•×‘ ×¡×™×›×•× ---
            try:
                 income_input_str = input(f"Enter estimated monthly income (default 12000): ")
                 income_input = float(income_input_str) if income_input_str else 12000
                 if not income_input_str: print("Using default income 12000.")
            except ValueError:
                 income_input = 12000
                 print("Invalid input, using default income 12000.")

            summary_df = calculate_summary(df_extracted.copy(), estimated_monthly_income=income_input) # Pass a copy to avoid modifying original df
            print("\nğŸ“‹ Financial Summary (V13):\n")
            # Display summary - format numbers if needed
            display(summary_df.style.format({"×¢×¨×š": lambda x: f"{x:,.0f}" if isinstance(x, (int, float)) and x > 1000 else x}))

        else:
            print(f"\n-> Could not extract structured data from {filename}.")



# -*- coding: utf-8 -*-
import pymupdf as fitz  # PyMuPDF library
import re
import pandas as pd
import plotly.express as px
from datetime import datetime
import logging
import unicodedata  # For normalization
from google.colab import files
from IPython.display import display
import traceback

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO)

# --- Helper Functions ---
def clean_number(text):
    if text is None: return None
    text = str(text).strip()
    text = re.sub(r'[â‚ª,]', '', text)
    if text.startswith('(') and text.endswith(')'): text = '-' + text[1:-1]
    if text.endswith('-'): text = '-' + text[:-1]
    try: return float(text)
    except ValueError: logging.warning(f"Could not convert '{text}' to float."); return None

def parse_date(date_str):
    if date_str is None: return None
    try: return datetime.strptime(date_str.strip(), '%d/%m/%Y')
    except ValueError:
        try: return datetime.strptime(date_str.strip(), '%d/%m/%y')
        except ValueError: logging.warning(f"Could not parse date: {date_str}"); return None

def normalize_text(text):
    return unicodedata.normalize('NFC', text)

# --- Main Extraction Function for Bank Hapoalim ---
def extract_transactions_from_pdf_hapoalim(pdf_content_bytes, filename_for_logging):
    transactions = []
    try:
        doc = fitz.open(stream=pdf_content_bytes, filetype="pdf")
    except Exception as e:
        logging.error(f"Failed to open/process PDF {filename_for_logging}: {e}", exc_info=True)
        return transactions

    date_pattern_end = re.compile(r"(\d{1,2}/\d{1,2}/\d{4})\s*$")
    balance_pattern_start = re.compile(r"^\s*(â‚ª?-?[\d,]+\.\d{2})")

    for page in doc:
        lines = page.get_text("text", sort=True).splitlines()
        for line in lines:
            original_line = line
            line_normalized = normalize_text(line.strip())
            if not line_normalized: continue

            date_match = date_pattern_end.search(original_line)
            if date_match:
                date_str = date_match.group(1)
                parsed_date = parse_date(date_str)
                if not parsed_date: continue

                balance_match = balance_pattern_start.search(original_line)
                if balance_match:
                    balance_str = balance_match.group(1)
                    balance = clean_number(balance_str)
                    if balance is not None:
                        transactions.append({
                            'Date': parsed_date,
                            'Balance': balance,
                            'SourceFile': filename_for_logging,
                            'LineText': original_line.strip()
                        })
    doc.close()
    return transactions

# --- Main Execution Block (Colab Version with Plotly) ---
if __name__ == "__main__":
    all_transactions = []
    print("Please upload your PDF bank statement file(s):")
    try:
        uploaded = files.upload()
        if uploaded:
            for filename, file_content in uploaded.items():
                try:
                    extracted_data = extract_transactions_from_pdf_hapoalim(file_content, filename)
                    all_transactions.extend(extracted_data)
                except Exception as e:
                    logging.error(f"Error processing file {filename}: {e}", exc_info=True)
                    traceback.print_exc()

            if not all_transactions:
                print("No transaction data could be extracted. Check logs/PDF format.")
            else:
                df = pd.DataFrame(all_transactions)
                df['Date'] = pd.to_datetime(df['Date'])
                df['Balance'] = pd.to_numeric(df['Balance'])
                df = df.sort_values(by=['Date', 'SourceFile', 'LineText'])
                df = df.drop_duplicates(subset='Date', keep='last').reset_index(drop=True)

                # Save to CSV
                df[['Date', 'Balance', 'SourceFile', 'LineText']].to_csv("extracted_balances_hapoalim.csv", index=False, encoding='utf-8-sig')
                print("\nâœ… Extracted data saved to extracted_balances_hapoalim.csv")

                # Plot with Plotly
                fig = px.line(
                    df,
                    x='Date',
                    y='Balance',
                    title='××’××ª ×™×ª×¨×ª ×—×©×‘×•×Ÿ ×œ××•×¨×š ×–××Ÿ (×‘× ×§ ×”×¤×•×¢×œ×™×)',
                    markers=True,
                    hover_data={'Date': True, 'Balance': ':.2f', 'LineText': True}
                )
                fig.update_layout(
                    xaxis_title='×ª××¨×™×š',
                    yaxis_title='×™×ª×¨×” ×‘×©"×—',
                    font=dict(family="David", size=14),
                    hoverlabel=dict(bgcolor="white", font_size=13),
                    template='plotly_white'
                )
                fig.update_yaxes(tickformat=",")
                fig.show()
                fig.write_html("balance_trend_interactive.html")
                print("ğŸ“ˆ ××™× ×˜×¨××§×˜×™×‘×™ × ×©××¨ ×›-balance_trend_interactive.html (× ×™×ª×Ÿ ×œ×”×•×¨×™×“) âœ…")
        else:
            print("No files were uploaded.")
    except Exception as e:
        print(f"Unexpected error: {e}")
        logging.error(f"Unexpected error: {e}", exc_info=True)
        traceback.print_exc()
    print("\nâœ”ï¸ Script finished.")

!pip install pdfplumber pandas

# -*- coding: utf-8 -*-
import pdfplumber
import pandas as pd
from google.colab import files
import io
import logging
import traceback
import unicodedata # For normalization
import re # For regular expressions
from datetime import datetime
import plotly.express as px
# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Helper Functions ---
def clean_number(text):
    """Cleans currency symbols, commas, and parentheses from number strings."""
    if text is None: return None
    text = str(text).strip()
    text = re.sub(r'[â‚ª,]', '', text)
    # Handle negative numbers in parentheses like (123.45)
    if text.startswith('(') and text.endswith(')'):
        text = '-' + text[1:-1]
    # Handle negative numbers ending with dash like 123.45-
    if text.endswith('-'):
         text = '-' + text[:-1]
    try: return float(text)
    except ValueError:
        # logging.warning(f"Could not convert '{text}' to float.")
        return None

def parse_date(date_str):
    """Parses date strings in DD/MM/YYYY or DD/MM/YY format."""
    if date_str is None: return None
    date_str = date_str.strip()
    try: return datetime.strptime(date_str, '%d/%m/%Y').date() # Use .date() to strip time
    except ValueError:
        try: return datetime.strptime(date_str, '%d/%m/%y').date()
        except ValueError:
            # logging.warning(f"Could not parse date: {date_str}")
            return None

def normalize_text(text):
    """Normalizes Unicode text to NFC form."""
    return unicodedata.normalize('NFC', text)

def reverse_hebrew_text(text):
    """Simple attempt to reverse Hebrew text that might be backwards."""
    if not text:
        return text
    # This is a very basic reversal and might not handle complex cases
    # like mixed LTR/RTL or punctuation perfectly.
    words = text.split()
    reversed_words = [word[::-1] for word in words] # Reverse each word
    return ' '.join(reversed_words[::-1]) # Reverse word order

# --- Specific Parsing Function for Discont Bank Format ---
def parse_discont_transaction_line(line_text):
    """
    Parses a single text line attempting to extract Discont Bank transaction data.

    Expected format (roughly): YYY.YY [Debit/Credit] [Ref] [Description] DD/MM/YYYY DD/MM/YYYY

    Args:
        line_text: The raw text string of a line.

    Returns:
        A dictionary with transaction details (Date, Value Date, Description,
        Debit, Credit, Reference, Balance) or None if it doesn't match the pattern.
    """
    line = line_text.strip()
    if not line:
        return None

    # Regex to capture the structure:
    # 1. Balance (starts with number, possibly -, may have comma, ends with .XX)
    # 2. Amount (possibly -, may have comma, ends with .XX)
    # 3. Potential Reference/Text
    # 4. Description/Text (can contain anything, needs careful boundary definition)
    # 5. Two dates (DD/MM/YYYY)

    # Adjusting regex based on observed pattern: Balance Amount Ref? Description Date1 Date2
    # Need to be careful with spaces and distinguishing Ref from Description.
    # Let's try capturing the dates and the initial numbers, then infer the rest.

    # Matches DD/MM/YYYY at the end, followed by space, then another DD/MM/YYYY
    date_pattern = re.compile(r"(\d{1,2}/\d{1,2}/\d{2,4})\s+(\d{1,2}/\d{1,2}/\d{2,4})$")
    date_match = date_pattern.search(line)

    if not date_match:
        # logging.debug(f"No date pattern match in line: {line}")
        return None # Not a transaction line if dates aren't at the end

    # Extract the date strings
    date_str2 = date_match.group(1) # The second date matched (often transaction date)
    date_str1 = date_match.group(2) # The first date matched (often value date)

    parsed_date = parse_date(date_str2) # Using the second date as the main one
    parsed_value_date = parse_date(date_str1)

    if not parsed_date: # Need at least the main transaction date
        return None

    # Remove the dates from the line to simplify parsing the rest
    line_before_dates = line[:date_match.start()].strip()

    # Now try to match the Balance and Amount at the start of the remaining line
    # Balance (Group 1), Amount (Group 2)
    # This regex looks for a number, followed by spaces, then another number at the start of the line
    # It assumes the structure is consistently <Balance> <Amount> ...
    balance_amount_pattern = re.compile(r"^([â‚ª\-,\d]+\.\d{2})\s+([â‚ª\-,\d]+\.\d{2})")
    balance_amount_match = balance_amount_pattern.search(line_before_dates)

    if not balance_amount_match:
        # logging.debug(f"No balance/amount pattern match after removing dates in line: {line_before_dates}")
        return None # Not a transaction line if numbers aren't at the start

    balance_str = balance_amount_match.group(1)
    amount_str = balance_amount_match.group(2)

    balance = clean_number(balance_str)
    amount = clean_number(amount_str)

    if balance is None or amount is None:
         # logging.debug(f"Failed to clean numbers from line: {line}")
         return None # Failed to parse numbers

    # Determine Debit/Credit based on amount sign
    debit = amount if amount < 0 else None
    credit = amount if amount > 0 else None

    # The rest of the line is the Reference and Description
    ref_desc_text = line_before_dates[balance_amount_match.end():].strip()

    # Now, how to split Ref and Description?
    # Looking at the examples, the reference (like 3715, 3716, 3717, 99219/3720)
    # often looks like a number or number/number combination and appears right after the amount.
    # The description follows the reference, sometimes with extra numbers/text at the end (like 11-066).

    # Let's try to split the ref_desc_text: The first "word" that looks like a potential reference
    # could be the reference, and the rest is description. This is heuristic and might fail.
    ref = None
    description = ref_desc_text

    # Regex for potential reference: numbers, maybe a slash, more numbers
    ref_pattern = re.compile(r"^([\w\-/]+)\s+(.*)$") # Match first word-like part, rest is description
    ref_match = ref_pattern.search(ref_desc_text)

    if ref_match:
        potential_ref = ref_match.group(1)
        rest_of_line = ref_match.group(2).strip()

        # Simple check: if the potential ref is mostly numbers or looks like a ref format
        if re.fullmatch(r"[\d\-/]+", potential_ref): # Checks if the potential ref is just digits, slashes, or hyphens
             ref = potential_ref
             description = rest_of_line
        else:
             # If the first word doesn't look like a reference, assume the whole remaining part is description
             ref = None # Or potential_ref if you want to keep it potentially
             description = ref_desc_text


    # Basic attempt to reverse potential backwards Hebrew text in description
    description = reverse_hebrew_text(description)


    return {
        'Date': parsed_date,
        'ValueDate': parsed_value_date,
        'Description': description,
        'Debit': abs(debit) if debit is not None else None, # Store debit as positive number
        'Credit': credit,
        'Reference': ref,
        'Balance': balance,
        'OriginalLine': line_text # Keep original line for debugging
    }


# --- Extraction Function using pdfplumber (extracting lines and parsing) ---
def extract_and_parse_discont_pdf(pdf_content_bytes, filename_for_logging):
    """
    Extracts lines and parses them using the Discont Bank specific parser.

    Args:
        pdf_content_bytes: The content of the PDF file as bytes.
        filename_for_logging: The original filename (for logging/identification).

    Returns:
        A list of dictionaries, where each dictionary is a parsed transaction.
    """
    transactions = []
    try:
        with pdfplumber.open(io.BytesIO(pdf_content_bytes)) as pdf:
            logging.info(f"Processing file: {filename_for_logging} with {len(pdf.pages)} pages.")
            for page_number, page in enumerate(pdf.pages):
                try:
                    text = page.extract_text(x_tolerance=2, y_tolerance=2) # Adjust tolerances if needed

                    if text:
                        lines = text.splitlines()
                        # logging.info(f"Extracted {len(lines)} lines from page {page_number + 1}. Attempting to parse...")
                        for line in lines:
                            parsed_transaction = parse_discont_transaction_line(line)
                            if parsed_transaction:
                                transactions.append(parsed_transaction)

                except Exception as e:
                    logging.warning(f"Error extracting or parsing text from page {page_number + 1} in {filename_for_logging}: {e}")

    except Exception as e:
        logging.error(f"Failed to open or process PDF file {filename_for_logging}: {e}", exc_info=True)
        return [] # Indicate failure

    return transactions


# --- Main Execution Block (Colab Version) ---
if __name__ == "__main__":
    print("Please upload your PDF bank statement file(s):")
    uploaded_files_data = {}
    try:
        # Use files.upload() to get file contents directly
        uploaded = files.upload()
        if uploaded:
            for filename, file_content in uploaded.items():
                 uploaded_files_data[filename] = file_content

            if not uploaded_files_data:
                print("No files were uploaded.")
            else:
                all_transactions = []
                for filename, content_bytes in uploaded_files_data.items():
                    # Assuming all uploaded files are Discont Bank format for this parser
                    transactions_from_file = extract_and_parse_discont_pdf(content_bytes, filename)
                    if transactions_from_file:
                         all_transactions.extend(transactions_from_file) # Add transactions from this file


                if not all_transactions:
                    print("\nğŸš¨ No transactions could be extracted using the Discont Bank parser.")
                    print("Please verify the PDF is a Discont Bank statement or the format has changed.")
                else:
                    print(f"\nâœ¨ Successfully extracted {len(all_transactions)} transactions.")
                    df_transactions = pd.DataFrame(all_transactions)

                    # Optional: Sort by date
                    df_transactions['Date'] = pd.to_datetime(df_transactions['Date'])
                    df_transactions = df_transactions.sort_values(by='Date').reset_index(drop=True)


                    print("\n--- Extracted Transactions (first 10 rows) ---")
                    display(df_transactions.head(10)) # Use display for nicer output in Colab

                    # Save to CSV
                    output_filename = "extracted_discont_transactions.csv"
                    df_transactions.to_csv(output_filename, index=False, encoding='utf-8-sig')
                    print(f"\nâœ… Extracted data saved to {output_filename} (can be downloaded from Files icon on left).")

                    # --- Optional Plotting ---
                    # Requires Balance column to be numeric and Date to be datetime
                    if 'Balance' in df_transactions.columns and pd.api.types.is_numeric_dtype(df_transactions['Balance']):
                         try:
                            fig = px.line(
                                df_transactions,
                                x='Date',
                                y='Balance',
                                title='Discont Bank Balance Trend Over Time',
                                markers=True,
                                hover_data={
                                    'Date': True,
                                    'Balance': ':.2f',
                                    'Description': True,
                                    'Debit': ':.2f',
                                    'Credit': ':.2f',
                                    'Reference': True,
                                    'OriginalLine': False # Don't show raw line in hover
                                    }
                            )
                            fig.update_layout(
                                xaxis_title='Date',
                                yaxis_title='Balance (NIS)',
                                font=dict(family="David", size=14), # Use a standard font
                                hoverlabel=dict(bgcolor="white", font_size=13),
                                template='plotly_white'
                            )
                            fig.update_yaxes(tickformat=",")
                            fig.show()
                            # fig.write_html("discont_balance_trend_interactive.html")
                            # print("ğŸ“ˆ Interactive plot saved as discont_balance_trend_interactive.html (can be downloaded).")
                         except Exception as plot_e:
                             logging.warning(f"Could not generate plot: {plot_e}")
                             print("\nâš ï¸ Could not generate plot. Check if 'Balance' column is numeric and 'Date' is datetime.")
                    else:
                         print("\nâš ï¸ Cannot generate plot. 'Balance' column is not numeric or 'Date' is not datetime.")


        else:
            print("No files were uploaded.")
    except Exception as e:
        print(f"\nğŸš¨ An unexpected error occurred during file upload or processing: {e}")
        logging.error(f"Unexpected error during upload/processing: {e}", exc_info=True)
        traceback.print_exc()

    print("\nâœ”ï¸ Script finished.")

# -*- coding: utf-8 -*-
import pdfplumber
import pandas as pd
from google.colab import files
import io
import logging
import traceback
import unicodedata # For normalization
import re # For regular expressions
from datetime import datetime
import plotly.express as px
import numpy as np # For handling potential NaN in balance calculation

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Helper Functions ---
# (Keep helper functions clean_number, parse_date, normalize_text, clean_transaction_amount as before)
def clean_transaction_amount(text):
    if text is None or pd.isna(text) or text == '': return None
    text = str(text).strip().replace('â‚ª', '').replace(',', '')
    if '.' not in text: return None # Require decimal for amount
    text = text.lstrip('\u200b')
    try:
        if text.count('.') > 1:
            parts = text.split('.')
            text = parts[0] + '.' + "".join(parts[1:])
        val = float(text)
        if abs(val) > 1_000_000: return None # Basic sanity check
        return val
    except ValueError: return None

def clean_number(text): # General number cleaning
    if text is None or pd.isna(text) or text == '': return None
    text = str(text).strip().replace('â‚ª', '').replace(',', '')
    text = text.lstrip('\u200b')
    try:
        if text.count('.') > 1:
            parts = text.split('.')
            text = parts[0] + '.' + "".join(parts[1:])
        return float(text)
    except ValueError: return None

def parse_date(date_str):
    if date_str is None or pd.isna(date_str) or not isinstance(date_str, str): return None
    date_str = date_str.strip()
    if not date_str: return None
    try: return datetime.strptime(date_str, '%d/%m/%Y').date()
    except ValueError:
        try: return datetime.strptime(date_str, '%d/%m/%y').date()
        except ValueError: return None

def normalize_text(text):
    if text is None or pd.isna(text): return None
    text = str(text).replace('\r', ' ').replace('\n', ' ')
    text = unicodedata.normalize('NFC', text.strip())
    if any('\u0590' <= char <= '\u05EA' for char in text):
       words = text.split()
       reversed_text = ' '.join(words[::-1])
       return reversed_text
    return text

# --- Specific Parsing Function - V2 (No change needed here for first line handling) ---
def parse_leumi_transaction_line_extracted_order_v2(line_text, previous_balance):
    """
    REVISED parser for the OBSERVED extracted order:
    Balance [Opt. Amount] Reference Desc Date ValueDate
    Distinguishes amount from reference more carefully.
    Returns parsed data, Debit/Credit might be None if previous_balance is None or mismatch.
    """
    line = line_text.strip()
    if not line: return None

    pattern = re.compile(
        r"^([\-\u200b\d,\.]+)\s+"           # 1: Balance
        r"(\d{1,3}(?:,\d{3})*\.\d{2})?\s*" # 2: Amount (Optional, specific format)
        r"(\S+)\s+"                        # 3: Reference
        r"(.*?)\s+"                        # 4: Description
        r"(\d{1,2}/\d{1,2}/\d{2,4})\s+"     # 5: Date
        r"(\d{1,2}/\d{1,2}/\d{2,4})$"       # 6: Value Date
    )
    match = pattern.match(line)
    if not match: return None

    balance_str, amount_str, reference_str, description_raw, date_str, value_date_str = match.groups()

    current_balance = clean_number(balance_str)
    parsed_date = parse_date(date_str)
    parsed_value_date = parse_date(value_date_str)
    description = normalize_text(description_raw)

    if parsed_date is None or current_balance is None: return None

    amount = clean_transaction_amount(amount_str)
    debit = None
    credit = None

    if amount is not None and amount != 0:
        if previous_balance is not None: # Only assign Debit/Credit if previous balance exists
            balance_diff = current_balance - previous_balance
            tolerance = 0.02
            if abs(balance_diff + amount) < tolerance: debit = amount
            elif abs(balance_diff - amount) < tolerance: credit = amount
            else: # Handle mismatch
                print(f"      WARN (parse): Amount ({amount}) mismatch balance change ({balance_diff:.2f}) Ref:{reference_str} Desc:{description} Line: {line[:80]}")
                # Fallback assignment based purely on direction if significant change
                if balance_diff < -tolerance: debit = amount
                elif balance_diff > tolerance: credit = amount
                # Debit/Credit remain None if mismatch and small change

    elif amount is None:
        # No amount parsed by regex, skip this line entirely
        # (could add logging here if needed)
        return None

    # Return data, Debit/Credit might be None
    return {
        'Date': parsed_date, 'ValueDate': parsed_value_date, 'Description': description,
        'Reference': reference_str,
        'Debit': debit, 'Credit': credit, 'Balance': current_balance,
        'OriginalLine': line_text
    }


# --- Extraction Function - Handles First Line Correctly ---
def extract_leumi_transactions_line_by_line(pdf_content_bytes, filename_for_logging):
    transactions = []
    print(f"\n--- Processing {filename_for_logging} using Line-by-Line Regex (Revised Extracted Order Parser V2 + First Line Handling) ---")
    try:
        with pdfplumber.open(io.BytesIO(pdf_content_bytes)) as pdf:
            # logging.info(f"Processing file: {filename_for_logging} with {len(pdf.pages)} pages.")
            previous_balance = None
            first_transaction_processed = False # Flag to track if we've processed the first valid line

            for page_number, page in enumerate(pdf.pages):
                print(f"\n[Page {page_number + 1}] Extracting text...")
                print("   (Using layout=True for text extraction)")
                text = page.extract_text(x_tolerance=2, y_tolerance=2, layout=True)
                if not text: continue
                lines = text.splitlines()
                print(f"  + Extracted {len(lines)} lines from page {page_number + 1}.")
                page_transactions_added = 0

                print("  Attempting to parse all non-empty lines...")
                for line_num, line_text in enumerate(lines):
                    cleaned_line = line_text.strip()
                    if not cleaned_line: continue

                    # Attempt parsing with the current previous_balance
                    parsed_transaction_data = parse_leumi_transaction_line_extracted_order_v2(cleaned_line, previous_balance)

                    if parsed_transaction_data:
                        # Successfully parsed line structure
                        current_balance = parsed_transaction_data['Balance']

                        if not first_transaction_processed:
                            # This is the first successfully parsed line
                            print(f"  L{line_num+1:03d}: INFO: First parsable line processed. Balance: {current_balance}. Setting initial balance, skipping add.")
                            previous_balance = current_balance # Set initial balance FOR NEXT ITERATION
                            first_transaction_processed = True # Mark flag
                            # Do NOT add this transaction to the list

                        else:
                            # This is a subsequent parsable line
                            # Check if Debit/Credit were successfully determined by the parser THIS time
                            if parsed_transaction_data['Debit'] is not None or parsed_transaction_data['Credit'] is not None:
                                transactions.append(parsed_transaction_data)
                                previous_balance = current_balance # Update previous balance for next line
                                page_transactions_added += 1
                                print(f"  L{line_num+1:03d}: SUCCESS: Transaction added. PrevBal updated to {previous_balance}")
                            else:
                                # Parsed structure OK, but Debit/Credit still None (e.g., balance mismatch warning occurred)
                                print(f"  L{line_num+1:03d}: SKIPPED: Parsed structure ok, but Debit/Credit undetermined (Balance mismatch?). PrevBal:{previous_balance}, CurrBal:{current_balance}")
                                # Update previous_balance anyway to allow potential recovery on the next line
                                previous_balance = current_balance


                    # else: # Line didn't even match basic structure / parse rules
                         # print(f"  L{line_num+1:03d}: FAILED/SKIPPED: Line did not match transaction pattern/rules: {cleaned_line[:100]}")


                print(f"  -> Added {page_transactions_added} transactions on page {page_number + 1}.")

    except Exception as e:
        logging.error(f"Failed to process PDF file {filename_for_logging}: {e}", exc_info=True)
        traceback.print_exc()
        return []

    print(f"\n--- Finished processing {filename_for_logging} ---")
    return transactions

# --- Main Execution Block (Colab Version) ---
# (Keep the main block __main__ as before)
if __name__ == "__main__":
    print("Please upload your Leumi Bank PDF statement file(s):")
    uploaded_files_data = {}
    try:
        uploaded = files.upload()
        if uploaded:
            for filename, file_content in uploaded.items():
                 uploaded_files_data[filename] = file_content

            if not uploaded_files_data:
                print("No files were uploaded.")
            else:
                combined_transactions_data = []
                for filename, content_bytes in uploaded_files_data.items():
                    print(f"\n===== Processing File: {filename} =====")
                    # Use the REVISED parser V2 with first line handling
                    transactions_from_file = extract_leumi_transactions_line_by_line(content_bytes, filename)
                    if transactions_from_file:
                         print(f"\n -> Found {len(transactions_from_file)} transactions in {filename}.")
                         combined_transactions_data.extend(transactions_from_file)
                    else:
                         print(f"\n -> No transactions extracted from {filename} using the revised parser.")


                if not combined_transactions_data:
                    print("\nğŸš¨ No transactions could be extracted from the uploaded file(s).")
                    print("   Check parser logic and first line handling.")
                else:
                    print(f"\nâœ¨ Successfully extracted a total of {len(combined_transactions_data)} transactions.")
                    df_transactions = pd.DataFrame(combined_transactions_data)

                    # --- Data Cleaning & Post-processing ---
                    df_transactions['Date'] = pd.to_datetime(df_transactions['Date'])
                    df_transactions['ValueDate'] = pd.to_datetime(df_transactions['ValueDate'], errors='coerce')
                    df_transactions['Balance'] = pd.to_numeric(df_transactions['Balance'], errors='coerce')
                    df_transactions['Debit'] = pd.to_numeric(df_transactions['Debit'], errors='coerce')
                    df_transactions['Credit'] = pd.to_numeric(df_transactions['Credit'], errors='coerce')

                    df_transactions = df_transactions.sort_values(
                         by=['Date', 'ValueDate', 'Balance'], ascending=[True, True, False], na_position='first'
                    ).reset_index(drop=True)


                    print("\n--- Extracted Transactions (first 10 rows) ---")
                    from google.colab.data_table import DataTable
                    display_cols = ['Date', 'ValueDate', 'Description', 'Reference', 'Debit', 'Credit', 'Balance']
                    display_df = df_transactions[display_cols].head(10).fillna('')
                    display(DataTable(display_df, include_index=False, num_rows_per_page=10))

                    print("\n--- Extracted Transactions (last 10 rows) ---")
                    display_df_tail = df_transactions[display_cols].tail(10).fillna('')
                    display(DataTable(display_df_tail, include_index=False, num_rows_per_page=10))

                    output_filename = "extracted_leumi_transactions_first_line_handled.csv" # Changed filename
                    df_transactions.to_csv(output_filename, index=False, encoding='utf-8-sig')
                    print(f"\nâœ… Extracted data saved to {output_filename}")
                    try:
                      files.download(output_filename)
                      print(f" -> Attempting to download '{output_filename}'...")
                    except Exception as download_err:
                      print(f" -> Could not automatically download the file. Error: {download_err}")

                    # --- Optional Plotting ---
                    df_plot = df_transactions.dropna(subset=['Date', 'Balance']).copy()
                    df_plot = df_plot.sort_values(by=['Date', 'ValueDate', 'Balance'], ascending=[True, True, False], na_position='first')

                    if not df_plot.empty:
                         try:
                            print("\nğŸ“Š Generating balance plot...")
                            fig = px.line(
                                df_plot, x='Date', y='Balance', title='Leumi Bank Balance Trend Over Time (First Line Handled)', markers=True,
                                hover_data={'Date': '|%d/%m/%Y', 'Balance': ':,.2f â‚ª', 'Description': True, 'Debit': ':,.2f', 'Credit': ':,.2f', 'Reference': True}
                            )
                            fig.update_layout(xaxis_title='Date', yaxis_title='Balance (â‚ª)', hovermode='x unified', template='plotly_white', font=dict(size=12))
                            fig.update_yaxes(tickprefix="â‚ª", tickformat=",")
                            fig.show()
                         except Exception as plot_e:
                             logging.warning(f"Could not generate plot: {plot_e}")
                             print(f"\nâš ï¸ Could not generate plot: {plot_e}")
                    else:
                         print("\nâš ï¸ Cannot generate plot. No valid 'Balance' or 'Date' data after processing.")

        else:
            print("No files were uploaded.")
    except Exception as e:
        print(f"\nğŸš¨ An unexpected error occurred: {e}")
        logging.error(f"Unexpected error: {e}", exc_info=True)
        traceback.print_exc()

    print("\nâœ”ï¸ Script finished.")





!pip install pdf2image

!pip install google-cloud-vision pdf2image pytesseract pandas google-colab opencv-python-headless requests transformers
!apt-get update
!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-heb

!pip install pdfplumber

!pip install pdf2image pytesseract transformers torch pandas requests google-colab
!apt-get update
!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-heb



!pip install python-doctr[torch]
!pip install tensorflow

!pip install doctr doctr.io

!pip install doctr.io pytesseract

